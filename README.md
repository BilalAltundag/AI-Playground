ğŸ§  AI & LLM Learning Journey

Bu depo (repository), Yapay Zeka ve BÃ¼yÃ¼k Dil Modelleri (LLM) Ã¼zerine yaptÄ±ÄŸÄ±m Ã§alÄ±ÅŸmalarÄ±, teorik analizleri ve sÄ±fÄ±rdan geliÅŸtirdiÄŸim modelleri iÃ§erir.

AmacÄ±m, sadece hazÄ±r API'leri kullanmak deÄŸil, "kaputun altÄ±ndaki" matematiÄŸi ve mimariyi (Backend) derinlemesine anlayarak Ã¶zelleÅŸtirilmiÅŸ AI Ã§Ã¶zÃ¼mleri Ã¼retmektir.

ğŸš€ Proje 1: Baby GPT - SÄ±fÄ±rdan Transformer EÄŸitimi

Bu projede, modern LLM'lerin (GPT-4, Gemini, Llama) temelini oluÅŸturan Transformer mimarisini PyTorch kullanarak sÄ±fÄ±rdan inÅŸa ettim ve eÄŸittim.

ğŸ¯ Projenin AmacÄ±

HazÄ±r kÃ¼tÃ¼phaneler (HuggingFace Trainer vb.) kullanmadan, ham PyTorch ile Self-Attention mekanizmasÄ±nÄ± kodlamak.

Tokenization, Embedding ve Positional Encoding sÃ¼reÃ§lerini manuel yÃ¶netmek.

Modeli bir diyalog veri seti ile eÄŸiterek basit bir Chatbot haline getirmek.

ğŸ› ï¸ KullanÄ±lan Teknolojiler

Core: Python, PyTorch (CUDA desteÄŸi ile)

Tokenizer: Tiktoken (OpenAI GPT-2 BPE)

Data: Hugging Face Datasets (knkarthick/dialogsum)

Deployment: Gradio (Web ArayÃ¼zÃ¼)

Visualization: Torchinfo, Matplotlib

ğŸ“š Teorik AltyapÄ± ve Notlar

Bu projeyi geliÅŸtirirken Ã¼zerine Ã§alÄ±ÅŸtÄ±ÄŸÄ±m temel kavramlar:

1. Neden RNN deÄŸil de Transformer?

Eskiden kullanÄ±lan RNN ve LSTM modelleri veriyi sÄ±rayla (seri) iÅŸliyordu. Bu durum unutkanlÄ±ÄŸa (uzun cÃ¼mlelerin baÅŸÄ±nÄ± unutma) ve yavaÅŸlÄ±ÄŸa (paralel iÅŸlem yapamama) yol aÃ§Ä±yordu. Transformerlar ise Dikkat (Attention) mekanizmasÄ± sayesinde cÃ¼mlenin tamamÄ±na aynÄ± anda odaklanabilir.

2. Self-Attention MekanizmasÄ± (Modelin Beyni)

Modelin kelimeler arasÄ±ndaki iliÅŸkiyi anlamasÄ±nÄ± saÄŸlayan algoritmadÄ±r. Bunu bir veritabanÄ± sorgusuna benzetebiliriz:

Query (Q - Sorgu): Token ne arÄ±yor? (Ã–rn: "Kedi" kelimesi bir eylem arÄ±yor)

Key (K - Anahtar): DiÄŸer kelimeler ne sunuyor? (Ã–rn: "Yemek", "Uyumak")

Value (V - DeÄŸer): EÄŸer eÅŸleÅŸme olursa ne kadar bilgi aktarÄ±lacak?

Ã–rnek: "Kedi mama yer" cÃ¼mlesinde; Kedi (Query) ile Yer (Key) arasÄ±ndaki matematiksel uyum (Dot Product) yÃ¼ksek Ã§Ä±kar. BÃ¶ylece model, kedinin beslendiÄŸini anlar.

3. Mimariden Kesitler

Projede kullandÄ±ÄŸÄ±m Multi-Head Attention yapÄ±sÄ±nÄ±n basitleÅŸtirilmiÅŸ mantÄ±ÄŸÄ±:

class Head(nn.Module):
    def forward(self, x):
        # Q, K, V vektÃ¶rlerini oluÅŸtur
        k = self.key(x)
        q = self.query(x)
        
        # Dikkat skorlarÄ±nÄ± hesapla (Matris Ã‡arpÄ±mÄ±)
        wei = q @ k.transpose(-2, -1) * (C**-0.5)
        
        # Maskeleme (GeleceÄŸi gÃ¶rmeyi engelle - Decoder Only)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        wei = F.softmax(wei, dim=-1)
        
        # DeÄŸerleri birleÅŸtir
        v = self.value(x)
        return wei @ v


ğŸ“Š Model KonfigÃ¼rasyonu

T4 GPU sÄ±nÄ±rlarÄ± dahilinde optimize edilmiÅŸ "Safe Pro" ayarlarÄ± kullanÄ±lmÄ±ÅŸtÄ±r:

Parametre

DeÄŸer

AÃ§Ä±klama

Model Tipi

Decoder-only Transformer

GPT mimarisi

Parametre SayÄ±sÄ±

~10 Milyon

Custom "Baby" boyutu

Context Window

192 Token

Modelin hafÄ±za derinliÄŸi

Embedding Size

384

NÃ¶ron katman geniÅŸliÄŸi

Layers (Derinlik)

6 Blok

Soyutlama seviyesi

Heads

6 Kafa

Paralel dikkat mekanizmasÄ±

ğŸ“‰ EÄŸitim SonuÃ§larÄ±

Model, DialogSum veri seti Ã¼zerinde 5000 adÄ±m boyunca eÄŸitilmiÅŸtir.

BaÅŸlangÄ±Ã§ Loss: ~4.5

BitiÅŸ Loss: ~0.5 (Model dil yapÄ±sÄ±nÄ± ve cevap verme mantÄ±ÄŸÄ±nÄ± Ã§Ã¶zdÃ¼)

(Buraya notebook'tan aldÄ±ÄŸÄ±n Loss grafiÄŸini ekleyebilirsin)

ğŸ’» NasÄ±l Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±r?

Bu repoyu klonlayÄ±n:

git clone [https://github.com/kullaniciadi/AI-Learning-Journey.git](https://github.com/kullaniciadi/AI-Learning-Journey.git)


Gerekli kÃ¼tÃ¼phaneleri kurun:

pip install torch tiktoken datasets gradio torchinfo tqdm


BabyGPT_Egitim.ipynb dosyasÄ±nÄ± Jupyter Lab veya Google Colab ile aÃ§Ä±p Ã§alÄ±ÅŸtÄ±rÄ±n.

Roadmap (Gelecek Hedefler)

[x] SÄ±fÄ±rdan Transformer Mimarisi (Baby GPT)

[ ] BÃ¼yÃ¼k bir modelin (Llama-3) Fine-Tuning iÅŸlemi

[ ] RAG (Retrieval Augmented Generation) ile dÃ¶kÃ¼man tabanlÄ± sohbet

[ ] Vision Transformer (ViT) ile gÃ¶rÃ¼ntÃ¼ iÅŸleme

Bu Ã§alÄ±ÅŸma, AI mimarisini derinlemesine Ã¶ÄŸrenmek amacÄ±yla oluÅŸturulmuÅŸtur.
